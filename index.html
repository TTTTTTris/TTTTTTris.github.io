<!DOCTYPE html>
<html>
<head>
<style>
body{background-color:gray}
div{margin:1em auto}
</style>
</head>
<body>
<div id="page1" style="position:relative;width:612pt;height:792pt;background-color:white">
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:21pt;left:275pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:14.944pt">Jiayi Tian</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:40pt;left:101pt"><span style="font-family:LMRoman8,serif;font-size:7.9701pt">+1 (805) 245 0298</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:40pt;left:177pt"><span style="font-family:LMRoman8,serif;font-size:7.9701pt">|</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:40pt;left:187pt"><span style="font-family:LMRoman8,serif;font-size:7.9701pt">jiayi_tian@ucsb.edu</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:40pt;left:270pt"><span style="font-family:LMRoman8,serif;font-size:7.9701pt">|</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:40pt;left:281pt"><span style="font-family:LMRoman8,serif;font-size:7.9701pt">github.com/ttttttris</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:40pt;left:363pt"><span style="font-family:LMRoman8,serif;font-size:7.9701pt">|</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:40pt;left:373pt"><span style="font-family:LMRoman8,serif;font-size:7.9701pt">linkedin.com/in/jiayi-tian-32b9652a5/</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:59pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt">Focus on efficient training and inference of LLMs (Tensor decomposition, Pruning, Quantization, Knowledge Distillation)</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:85pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt;color:#4471c4">EDUCATION</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:98pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.9589pt">University of California, Santa Barbara</span></b><span style="font-family:TeXGyreTermesX,serif;font-size:10.9589pt">,</span><i><span style="font-family:TeXGyreTermesX,serif;font-size:10.9589pt"> Ph.D. in Computer Engineering</span></i><span style="font-family:TeXGyreTermesX,serif;font-size:10.9589pt"> | CA, USA</span><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.9589pt"> 3.9/4.0</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:100pt;left:515pt"><span style="font-family:TeXGyreTermesX,serif;font-size:9.4645pt;color:#414141">Fall 2023 - ongoing</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:110pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.9589pt">Nanjing University</span></b><span style="font-family:TeXGyreTermesX,serif;font-size:10.9589pt">,</span><i><span style="font-family:TeXGyreTermesX,serif;font-size:10.9589pt"> B.Eng. in VLSI Design &amp; System Integration</span></i><span style="font-family:TeXGyreTermesX,serif;font-size:10.9589pt"> | China</span><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.9589pt"> 4.5/5.0</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:112pt;left:513pt"><span style="font-family:TeXGyreTermesX,serif;font-size:9.4645pt;color:#414141">Fall 2019 - Jun 2023</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:143pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt;color:#4471c4">INDUSTRIAL EXPERIENCE</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:160pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt">Intel Corporation,</span></b><i><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt"> Research Intern</span></i><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt"> | Portland, OR</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:160pt;left:474pt"><span style="font-family:LMRoman10,serif;font-size:9.4645pt">June. 2024 - Sep. 2024</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:186pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Proposed a tensor-compressed LLM training accelerator using FPGA with optimized compute ordering, dataflow,</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:196pt;left:51pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">and memory allocation.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:173pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Achieved up to</span><span style="font-family:LatinModernMath,serif;font-size:10.4608pt"> 48&#xd7;</span><span style="font-family:LMRoman10,serif;font-size:10.4608pt"> memory efficiency and</span><span style="font-family:LatinModernMath,serif;font-size:10.4608pt"> 3.6&#xd7;</span><span style="font-family:LMRoman10,serif;font-size:10.4608pt"> energy efficiency compared to Nvidia RTX 3090.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:209pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Resulting paper under review at IEEE TCAD.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:226pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt">AMD-Xilinx Technology,</span></b><i><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt"> Co-Op/Intern</span></i><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt"> | Beijing, China</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:226pt;left:481pt"><span style="font-family:LMRoman10,serif;font-size:9.4645pt">June 2023 - Sep 2023</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:252pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Developed a C++/HLS Transformer training framework with custom tensorized linear layers and nonlinear oper-</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:263pt;left:51pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">ations for LLM acceleration.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:239pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Achieved</span><span style="font-family:LatinModernMath,serif;font-size:10.4608pt"> 30&#xd7; &#x223c; 52&#xd7;</span><span style="font-family:LMRoman10,serif;font-size:10.4608pt"> saving in model size for end-to-end Transformer training.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:287pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt;color:#4471c4">SKILLS &amp; RESEARCH INTERESTS</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:302pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt">Languages &amp; Tools</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:302pt;left:132pt"><span style="font-family:LMRoman10,serif;font-size:9.9626pt">Python, PyTorch, TensorFlow, Huggingface, C/C++, High-level Synthesis (HLS), Vivado/Vitis/XRT</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:332pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt">ML &amp; NLP</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:326pt;left:132pt"><span style="font-family:LMRoman10,serif;font-size:9.9626pt">Large Language Models (LLMs), Efficient Training/Inference Speedup (Model Compression, Pruning,</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:338pt;left:132pt"><span style="font-family:LMRoman10,serif;font-size:9.9626pt">SVD/Tensor-decomposition, Distillation, Quantization)</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:364pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt;color:#4471c4">PUBLICATIONS &amp; PREPRINTS</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:387pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt;color:#414141">BEBERT: Efficient and robust binary ensemble BERT</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:400pt;left:42pt"><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt;color:#414141">Tian, Jiayi, Chao Fang, Haonan Wang, and Zhongfeng Wang, ICASSP 2023-2023 IEEE International Conference on Acoustics,</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:412pt;left:42pt"><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt;color:#414141">Speech and Signal Processing (ICASSP). IEEE, 2023.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:429pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt;color:#414141">Ultra Memory-Efficient On-FPGA Training of Transformers via Tensor-Compressed Optimization</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:441pt;left:42pt"><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt;color:#414141">Jiayi Tian, Jinming Lu, Hai Li, Xiangwei Wang, Cong (Callie) Hao, Ian Young, Zheng Zhang, under review at IEEE Transactions on</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:453pt;left:42pt"><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt;color:#414141">Computer-Aided Design of Integrated Circuits and Systems. arXiv preprint arXiv:2501.06663.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:471pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt;color:#414141">FETTA: Flexible and Efficient Hardware Accelerator for Tensorized Neural Network Training</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:483pt;left:42pt"><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt;color:#414141">Jinming Lu, Jiayi Tian, Hai Li, Ian Young, Zheng Zhang, under review at IEEE Transactions on Computer-Aided Design of Integrated</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:495pt;left:42pt"><span style="font-family:TeXGyreTermesX,serif;font-size:9.9626pt;color:#414141">Circuits and Systems.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:521pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt;color:#4471c4">RESEARCH PROJECTS</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:540pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt">Structural Pruning for Efficient LLM Inference leveraging Tensor Decomposition</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:540pt;left:485pt"><span style="font-family:LMRoman10,serif;font-size:9.4645pt">Aug. 2024 - Current</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:553pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Explored structural pruning of LLM that leveraging the low-rankness of data to prune model weights.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:566pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Proposed head-wise SVD, joint PCA, and Nystr&#xf6;m methods across decoder modules.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:579pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Demonstrated superior results on LLaMA and other large-scale LLMs over existing pruning baselines.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:595pt;left:42pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt">Training Accelerator Design for Tensor-Compressed Transformer Models</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:595pt;left:478pt"><span style="font-family:LMRoman10,serif;font-size:9.4645pt">Sep. 2023 - Dec. 2024</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:608pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Designed a tensor-decomposition-based training scheme that reduces parameter count by</span><span style="font-family:LatinModernMath,serif;font-size:10.4608pt"> 30&#xd7; &#x223c; 52&#xd7;</span><span style="font-family:LMRoman10,serif;font-size:10.4608pt">.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:621pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Introduced bidirectional tensor contraction to enhance memory and compute efficiency, especially in long-sequence</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:632pt;left:51pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">training and inference.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:645pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Built an HLS-based Transformer training engine achieving up to</span><span style="font-family:LatinModernMath,serif;font-size:10.4608pt"> 48&#xd7;</span><span style="font-family:LMRoman10,serif;font-size:10.4608pt"> memory efficiency and</span><span style="font-family:LatinModernMath,serif;font-size:10.4608pt"> 3.6&#xd7;</span><span style="font-family:LMRoman10,serif;font-size:10.4608pt"> energy efficiency</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:656pt;left:51pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">compared with Nvidia RTX 3090.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:672pt;left:45pt"><b><span style="font-family:TeXGyreTermesX,serif;font-size:10.4608pt">Binary-Quantized Ensemble LLM for Fast and Robust Language Model Inference</span></b></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:672pt;left:476pt"><span style="font-family:LMRoman10,serif;font-size:9.4645pt">Apr. 2021 - June. 2023</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:685pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Developed BEBERT, a novel quantization-ensemble strategy enabling efficient and accurate 1-bit BERT inference.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:698pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Leveraged efficient knowledge distillation strategy for high training efficiency.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:711pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Achieved</span><span style="font-family:LatinModernMath,serif;font-size:10.4608pt"> 13&#xd7;</span><span style="font-family:LMRoman10,serif;font-size:10.4608pt"> model size reduction and</span><span style="font-family:LatinModernMath,serif;font-size:10.4608pt"> 15&#xd7;</span><span style="font-family:LMRoman10,serif;font-size:10.4608pt"> compute savings over standard BERT with minimal accuracy loss.</span></p>
<p style="position:absolute;white-space:pre;margin:0;padding:0;top:724pt;left:41pt"><span style="font-family:LMRoman10,serif;font-size:10.4608pt">&#x2022; Proposed early-exit inference variant, further cutting compute by</span><span style="font-family:LatinModernMath,serif;font-size:10.4608pt"> 20% &#x223c; 40%</span><span style="font-family:LMRoman10,serif;font-size:10.4608pt"> on GLUE benchmark.</span></p>
</div>
</body>
</html>

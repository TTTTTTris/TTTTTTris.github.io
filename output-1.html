<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>Page 1</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft00{font-size:22px;font-family:PWGBDA+TeXGyreTermesX;color:#000000;}
	.ft01{font-size:11px;font-family:KFEWCP+LMRoman8;color:#000000;}
	.ft02{font-size:14px;font-family:PWGBDA+TeXGyreTermesX;color:#000000;}
	.ft03{font-size:15px;font-family:PWGBDA+TeXGyreTermesX;color:#4471c4;}
	.ft04{font-size:16px;font-family:PWGBDA+TeXGyreTermesX;color:#000000;}
	.ft05{font-size:16px;font-family:GTJHUB+TeXGyreTermesX;color:#000000;}
	.ft06{font-size:16px;font-family:UMHFGK+TeXGyreTermesX;color:#000000;}
	.ft07{font-size:14px;font-family:GTJHUB+TeXGyreTermesX;color:#414141;}
	.ft08{font-size:15px;font-family:PWGBDA+TeXGyreTermesX;color:#000000;}
	.ft09{font-size:15px;font-family:UMHFGK+TeXGyreTermesX;color:#000000;}
	.ft010{font-size:15px;font-family:GTJHUB+TeXGyreTermesX;color:#000000;}
	.ft011{font-size:14px;font-family:YCMHGQ+LMRoman10;color:#000000;}
	.ft012{font-size:15px;font-family:YCMHGQ+LMRoman10;color:#000000;}
	.ft013{font-size:15px;font-family:GSVHDI+LatinModernMath;color:#000000;}
	.ft014{font-size:14px;font-family:PWGBDA+TeXGyreTermesX;color:#414141;}
	.ft015{font-size:14px;line-height:17px;font-family:YCMHGQ+LMRoman10;color:#000000;}
	.ft016{font-size:14px;line-height:17px;font-family:GTJHUB+TeXGyreTermesX;color:#414141;}
	.ft017{font-size:15px;line-height:16px;font-family:YCMHGQ+LMRoman10;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page1-div" style="position:relative;width:918px;height:1188px;">
<img width="918" height="1188" src="output001.png" alt="background image"/>
<p style="position:absolute;top:40px;left:413px;white-space:nowrap" class="ft00"><b>Jiayi&#160;Tian</b></p>
<p style="position:absolute;top:65px;left:152px;white-space:nowrap" class="ft01"><a href="tel:+1 (805) 245 0298">+1&#160;(805)&#160;245&#160;0298</a></p>
<p style="position:absolute;top:65px;left:266px;white-space:nowrap" class="ft01">|&#160;<a href="mailto:jiayi_tian@ucsb.edu">jiayi_tian@ucsb.edu&#160;</a>|&#160;<a href="https://github.com/ttttttris">github.com/ttttttris&#160;</a>|</p>
<p style="position:absolute;top:65px;left:560px;white-space:nowrap" class="ft01"><a href="https://www.linkedin.com/in/jiayi-tian-32b9652a5/">linkedin.com/in/jiayi-tian-32b9652a5/</a></p>
<p style="position:absolute;top:94px;left:64px;white-space:nowrap" class="ft02"><b>Focus&#160;on&#160;eﬀicient&#160;training&#160;and&#160;inference&#160;of&#160;LLMs&#160;(Low-rank&#160;decomposition,&#160;Pruning,&#160;Quantization,&#160;Knowledge&#160;Distillation)</b></p>
<p style="position:absolute;top:133px;left:64px;white-space:nowrap" class="ft03"><b>EDUCATION</b></p>
<p style="position:absolute;top:153px;left:64px;white-space:nowrap" class="ft04"><b>University&#160;of&#160;California,&#160;Santa&#160;Barbara</b></p>
<p style="position:absolute;top:154px;left:341px;white-space:nowrap" class="ft05">,</p>
<p style="position:absolute;top:153px;left:350px;white-space:nowrap" class="ft06"><i>Ph.D.&#160;in&#160;Computer&#160;Engineering</i></p>
<p style="position:absolute;top:154px;left:564px;white-space:nowrap" class="ft05">|&#160;CA,&#160;USA</p>
<p style="position:absolute;top:153px;left:638px;white-space:nowrap" class="ft04"><b>3.9/4.0</b></p>
<p style="position:absolute;top:155px;left:773px;white-space:nowrap" class="ft07">Fall&#160;2023&#160;-&#160;ongoing</p>
<p style="position:absolute;top:171px;left:64px;white-space:nowrap" class="ft04"><b>Nanjing&#160;University</b></p>
<p style="position:absolute;top:171px;left:196px;white-space:nowrap" class="ft05">,</p>
<p style="position:absolute;top:171px;left:205px;white-space:nowrap" class="ft06"><i>B.Eng.&#160;in&#160;VLSI&#160;Design&#160;&amp;&#160;System&#160;Integration</i></p>
<p style="position:absolute;top:171px;left:503px;white-space:nowrap" class="ft05">|&#160;China</p>
<p style="position:absolute;top:171px;left:553px;white-space:nowrap" class="ft04"><b>4.5/5.0</b></p>
<p style="position:absolute;top:173px;left:771px;white-space:nowrap" class="ft07">Fall&#160;2019&#160;-&#160;Jun&#160;2023</p>
<p style="position:absolute;top:212px;left:64px;white-space:nowrap" class="ft03"><b>INDUSTRIAL&#160;EXPERIENCE</b></p>
<p style="position:absolute;top:238px;left:64px;white-space:nowrap" class="ft08"><b>Intel&#160;Corporation,</b></p>
<p style="position:absolute;top:238px;left:191px;white-space:nowrap" class="ft09"><i>Research&#160;Intern</i></p>
<p style="position:absolute;top:238px;left:294px;white-space:nowrap" class="ft010">|&#160;Portland,&#160;OR</p>
<p style="position:absolute;top:239px;left:711px;white-space:nowrap" class="ft011">June.&#160;2024&#160;-&#160;Sep.&#160;2024</p>
<p style="position:absolute;top:257px;left:58px;white-space:nowrap" class="ft012">•&#160;Proposed&#160;a&#160;tensor-compressed&#160;LLM&#160;training&#160;accelerator&#160;using&#160;FPGA&#160;with&#160;optimized&#160;compute&#160;ordering,&#160;dataflow,</p>
<p style="position:absolute;top:272px;left:77px;white-space:nowrap" class="ft012">and&#160;memory&#160;allocation.</p>
<p style="position:absolute;top:289px;left:58px;white-space:nowrap" class="ft012">•&#160;Achieved&#160;up&#160;to</p>
<p style="position:absolute;top:289px;left:186px;white-space:nowrap" class="ft013">48×</p>
<p style="position:absolute;top:289px;left:219px;white-space:nowrap" class="ft012">memory&#160;eﬀiciency&#160;and</p>
<p style="position:absolute;top:289px;left:378px;white-space:nowrap" class="ft013">3.6×</p>
<p style="position:absolute;top:289px;left:415px;white-space:nowrap" class="ft012">energy&#160;eﬀiciency&#160;compared&#160;to&#160;Nvidia&#160;RTX&#160;3090.</p>
<p style="position:absolute;top:305px;left:58px;white-space:nowrap" class="ft012">•&#160;Resulting&#160;paper&#160;under&#160;review&#160;at&#160;IEEE&#160;TCAD.</p>
<p style="position:absolute;top:333px;left:64px;white-space:nowrap" class="ft08"><b>AMD-Xilinx&#160;Technology,</b></p>
<p style="position:absolute;top:333px;left:234px;white-space:nowrap" class="ft09"><i>Co-Op/Intern</i></p>
<p style="position:absolute;top:333px;left:324px;white-space:nowrap" class="ft010">|&#160;Beijing,&#160;China</p>
<p style="position:absolute;top:334px;left:722px;white-space:nowrap" class="ft011">June&#160;2023&#160;-&#160;Sep&#160;2023</p>
<p style="position:absolute;top:353px;left:58px;white-space:nowrap" class="ft012">•&#160;Developed&#160;a&#160;C++/HLS&#160;Transformer&#160;training&#160;framework&#160;with&#160;custom&#160;tensorized&#160;linear&#160;layers&#160;and&#160;nonlinear&#160;oper-</p>
<p style="position:absolute;top:369px;left:77px;white-space:nowrap" class="ft012">ations&#160;for&#160;LLM&#160;acceleration.</p>
<p style="position:absolute;top:386px;left:58px;white-space:nowrap" class="ft012">•&#160;Achieved</p>
<p style="position:absolute;top:386px;left:144px;white-space:nowrap" class="ft013">30×&#160;∼&#160;52×</p>
<p style="position:absolute;top:386px;left:226px;white-space:nowrap" class="ft012">saving&#160;in&#160;model&#160;size&#160;for&#160;end-to-end&#160;Transformer&#160;training.</p>
<p style="position:absolute;top:428px;left:64px;white-space:nowrap" class="ft03"><b>SKILLS&#160;&amp;&#160;RESEARCH&#160;INTERESTS</b></p>
<p style="position:absolute;top:450px;left:64px;white-space:nowrap" class="ft02"><b>Languages&#160;&amp;&#160;Tools</b></p>
<p style="position:absolute;top:450px;left:199px;white-space:nowrap" class="ft011">Python,&#160;PyTorch,&#160;TensorFlow,&#160;Huggingface,&#160;C/C++,&#160;High-level&#160;Synthesis&#160;(HLS),&#160;Vivado/Vitis/XRT</p>
<p style="position:absolute;top:477px;left:64px;white-space:nowrap" class="ft02"><b>ML&#160;&amp;&#160;NLP</b></p>
<p style="position:absolute;top:468px;left:199px;white-space:nowrap" class="ft015">Large&#160;Language&#160;Models&#160;(LLMs),&#160;Eﬀicient&#160;Training/Inference&#160;(Model&#160;Compression,&#160;Pruning,&#160;Low-rank<br/>decomposition,&#160;Distillation,&#160;Quantization)</p>
<p style="position:absolute;top:525px;left:64px;white-space:nowrap" class="ft03"><b>PUBLICATIONS&#160;&amp;&#160;PREPRINTS</b></p>
<p style="position:absolute;top:560px;left:64px;white-space:nowrap" class="ft014"><b>FLAT-LLM:&#160;Fine-grained&#160;Low-rank&#160;Activation&#160;Space&#160;Transformation&#160;for&#160;Large&#160;Language&#160;Model&#160;Compression</b></p>
<p style="position:absolute;top:578px;left:64px;white-space:nowrap" class="ft07">Jiayi&#160;Tian,&#160;Ryan&#160;Solgi,&#160;Jinming&#160;Lu,&#160;Yifan&#160;Yang,&#160;Hai&#160;Li,&#160;Zheng&#160;Zhang,&#160;under&#160;review&#160;at&#160;ARR&#160;May,&#160;2025.&#160;<a href="https://arxiv.org/pdf/2505.23966">arXiv&#160;preprint</a>.</p>
<p style="position:absolute;top:605px;left:64px;white-space:nowrap" class="ft014"><b>FETTA:&#160;Flexible&#160;and&#160;Eﬀicient&#160;Hardware&#160;Accelerator&#160;for&#160;Tensorized&#160;Neural&#160;Network&#160;Training</b></p>
<p style="position:absolute;top:623px;left:64px;white-space:nowrap" class="ft016">Jinming&#160;Lu,&#160;Jiayi&#160;Tian,&#160;Hai&#160;Li,&#160;Ian&#160;Young,&#160;Zheng&#160;Zhang,&#160;under&#160;review&#160;at&#160;IEEE&#160;Transactions&#160;on&#160;Computer-Aided&#160;Design&#160;of&#160;Integrated<br/>Circuits&#160;and&#160;Systems.&#160;<a href="https://arxiv.org/pdf/2504.06474">arXiv&#160;preprint</a>.</p>
<p style="position:absolute;top:667px;left:64px;white-space:nowrap" class="ft014"><b>Ultra&#160;Memory-Eﬀicient&#160;On-FPGA&#160;Training&#160;of&#160;Transformers&#160;via&#160;Tensor-Compressed&#160;Optimization</b></p>
<p style="position:absolute;top:685px;left:64px;white-space:nowrap" class="ft016">Jiayi&#160;Tian,&#160;Jinming&#160;Lu,&#160;Hai&#160;Li,&#160;Xiangwei&#160;Wang,&#160;Cong&#160;(Callie)&#160;Hao,&#160;Ian&#160;Young,&#160;Zheng&#160;Zhang,&#160;under&#160;review&#160;at&#160;IEEE&#160;Transactions&#160;on<br/>Computer-Aided&#160;Design&#160;of&#160;Integrated&#160;Circuits&#160;and&#160;Systems.&#160;<a href="https://arxiv.org/pdf/2501.06663">arXiv&#160;preprint</a>.</p>
<p style="position:absolute;top:730px;left:64px;white-space:nowrap" class="ft014"><b>BEBERT:&#160;Eﬀicient&#160;and&#160;robust&#160;binary&#160;ensemble&#160;BERT</b></p>
<p style="position:absolute;top:748px;left:64px;white-space:nowrap" class="ft016">Tian,&#160;Jiayi,&#160;Chao&#160;Fang,&#160;Haonan&#160;Wang,&#160;and&#160;Zhongfeng&#160;Wang,&#160;ICASSP&#160;2023-2023&#160;IEEE&#160;International&#160;Conference&#160;on&#160;Acoustics,<br/>Speech&#160;and&#160;Signal&#160;Processing&#160;(ICASSP).&#160;IEEE,&#160;2023.</p>
<p style="position:absolute;top:805px;left:64px;white-space:nowrap" class="ft03"><b>RESEARCH&#160;PROJECTS</b></p>
<p style="position:absolute;top:834px;left:64px;white-space:nowrap" class="ft08"><b>Structural&#160;Pruning&#160;for&#160;Eﬀicient&#160;LLM&#160;Inference&#160;via&#160;Low-rank&#160;Decomposition</b></p>
<p style="position:absolute;top:835px;left:728px;white-space:nowrap" class="ft011">Aug.&#160;2024&#160;-&#160;Current</p>
<p style="position:absolute;top:854px;left:58px;white-space:nowrap" class="ft012">•&#160;Developed&#160;a&#160;training-free,&#160;fine-grained&#160;compression&#160;method&#160;that&#160;leverages&#160;the&#160;low-rank&#160;structure&#160;of&#160;the&#160;activation</p>
<p style="position:absolute;top:869px;left:77px;white-space:nowrap" class="ft012">space&#160;to&#160;transform&#160;and&#160;compress&#160;model&#160;weights.</p>
<p style="position:absolute;top:885px;left:58px;white-space:nowrap" class="ft012">•&#160;Proposed&#160;a&#160;novel&#160;rank&#160;selection&#160;algorithm&#160;based&#160;on&#160;component&#160;importance,&#160;enabling&#160;seamless&#160;integration&#160;with</p>
<p style="position:absolute;top:900px;left:77px;white-space:nowrap" class="ft012">existing&#160;low-rank&#160;LLM&#160;compression&#160;pipelines.</p>
<p style="position:absolute;top:918px;left:58px;white-space:nowrap" class="ft012">•&#160;Achieved&#160;strong&#160;performance&#160;on&#160;LLaMA-2&#160;and&#160;Mistral&#160;models&#160;with&#160;minimal&#160;calibration&#160;overhead&#160;(within&#160;minutes),</p>
<p style="position:absolute;top:933px;left:77px;white-space:nowrap" class="ft012">validated&#160;across&#160;language&#160;modeling&#160;and&#160;downstream&#160;tasks.</p>
<p style="position:absolute;top:962px;left:64px;white-space:nowrap" class="ft08"><b>Training&#160;Accelerator&#160;Design&#160;for&#160;Tensor-Compressed&#160;Transformer&#160;Models</b></p>
<p style="position:absolute;top:963px;left:717px;white-space:nowrap" class="ft011">Sep.&#160;2023&#160;-&#160;Dec.&#160;2024</p>
<p style="position:absolute;top:982px;left:58px;white-space:nowrap" class="ft012">•&#160;Designed&#160;a&#160;tensor-compressed&#160;training&#160;scheme&#160;for&#160;Transformer&#160;models&#160;that&#160;reduces&#160;model&#160;size&#160;by</p>
<p style="position:absolute;top:982px;left:755px;white-space:nowrap" class="ft013">30&#160;∼&#160;52×</p>
<p style="position:absolute;top:982px;left:819px;white-space:nowrap" class="ft012">.</p>
<p style="position:absolute;top:999px;left:58px;white-space:nowrap" class="ft012">•&#160;Introduced&#160;bidirectional&#160;tensor&#160;contraction&#160;to&#160;enhance&#160;memory&#160;and&#160;compute&#160;eﬀiciency,&#160;especially&#160;in&#160;long-sequence</p>
<p style="position:absolute;top:1014px;left:77px;white-space:nowrap" class="ft012">training&#160;and&#160;inference.</p>
<p style="position:absolute;top:1030px;left:58px;white-space:nowrap" class="ft012">•&#160;Built&#160;an&#160;HLS-based&#160;Transformer&#160;training&#160;engine&#160;achieving&#160;up&#160;to</p>
<p style="position:absolute;top:1030px;left:518px;white-space:nowrap" class="ft013">48×</p>
<p style="position:absolute;top:1030px;left:550px;white-space:nowrap" class="ft012">memory&#160;eﬀiciency&#160;and</p>
<p style="position:absolute;top:1030px;left:706px;white-space:nowrap" class="ft013">3.6×</p>
<p style="position:absolute;top:1030px;left:743px;white-space:nowrap" class="ft012">energy&#160;eﬀiciency</p>
<p style="position:absolute;top:1045px;left:77px;white-space:nowrap" class="ft012">compared&#160;with&#160;Nvidia&#160;RTX&#160;3090.</p>
<p style="position:absolute;top:1068px;left:69px;white-space:nowrap" class="ft08"><b>Binary-Quantized&#160;Ensemble&#160;LLM&#160;for&#160;Fast&#160;and&#160;Robust&#160;Language&#160;Model&#160;Inference</b></p>
<p style="position:absolute;top:1069px;left:714px;white-space:nowrap" class="ft011">Apr.&#160;2021&#160;-&#160;June.&#160;2023</p>
<p style="position:absolute;top:1088px;left:63px;white-space:nowrap" class="ft017">•&#160;Developed&#160;BEBERT,&#160;a&#160;novel&#160;quantization-ensemble&#160;strategy&#160;enabling&#160;eﬀicient&#160;and&#160;accurate&#160;1-bit&#160;BERT&#160;inference.<br/>•&#160;Leveraged&#160;eﬀicient&#160;knowledge&#160;distillation&#160;strategy&#160;for&#160;high&#160;training&#160;eﬀiciency.<br/>•&#160;Achieved</p>
<p style="position:absolute;top:1121px;left:149px;white-space:nowrap" class="ft013">13×</p>
<p style="position:absolute;top:1121px;left:182px;white-space:nowrap" class="ft012">model&#160;size&#160;reduction&#160;and</p>
<p style="position:absolute;top:1121px;left:359px;white-space:nowrap" class="ft013">15×</p>
<p style="position:absolute;top:1121px;left:392px;white-space:nowrap" class="ft012">compute&#160;savings&#160;over&#160;standard&#160;BERT&#160;with&#160;minimal&#160;accuracy&#160;loss.</p>
<p style="position:absolute;top:1139px;left:63px;white-space:nowrap" class="ft012">•&#160;Proposed&#160;early-exit&#160;inference&#160;variant,&#160;further&#160;cutting&#160;compute&#160;by</p>
<p style="position:absolute;top:1139px;left:537px;white-space:nowrap" class="ft013">20%&#160;∼&#160;40%</p>
<p style="position:absolute;top:1139px;left:620px;white-space:nowrap" class="ft012">on&#160;GLUE&#160;benchmark.</p>
</div>
</body>
</html>

<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>Page 1</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft00{font-size:22px;font-family:WIWSXX+TeXGyreTermesX;color:#000000;}
	.ft01{font-size:12px;font-family:PTRFWD+LMRoman8;color:#000000;}
	.ft02{font-size:15px;font-family:WIWSXX+TeXGyreTermesX;color:#000000;}
	.ft03{font-size:16px;font-family:WIWSXX+TeXGyreTermesX;color:#4471c4;}
	.ft04{font-size:16px;font-family:WIWSXX+TeXGyreTermesX;color:#000000;}
	.ft05{font-size:16px;font-family:FKQICN+TeXGyreTermesX;color:#000000;}
	.ft06{font-size:16px;font-family:ZGVNVQ+TeXGyreTermesX;color:#000000;}
	.ft07{font-size:14px;font-family:FKQICN+TeXGyreTermesX;color:#414141;}
	.ft08{font-size:14px;font-family:ISGITX+LMRoman10;color:#000000;}
	.ft09{font-size:16px;font-family:ISGITX+LMRoman10;color:#000000;}
	.ft010{font-size:16px;font-family:CPCRSF+LatinModernMath;color:#000000;}
	.ft011{font-size:15px;font-family:ISGITX+LMRoman10;color:#000000;}
	.ft012{font-size:15px;font-family:WIWSXX+TeXGyreTermesX;color:#414141;}
	.ft013{font-size:15px;font-family:FKQICN+TeXGyreTermesX;color:#414141;}
	.ft014{font-size:15px;line-height:17px;font-family:ISGITX+LMRoman10;color:#000000;}
	.ft015{font-size:15px;line-height:17px;font-family:FKQICN+TeXGyreTermesX;color:#414141;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page1-div" style="position:relative;width:918px;height:1188px;">
<img width="918" height="1188" src="output001.png" alt="background image"/>
<p style="position:absolute;top:40px;left:413px;white-space:nowrap" class="ft00"><b>Jiayi&#160;Tian</b></p>
<p style="position:absolute;top:65px;left:152px;white-space:nowrap" class="ft01"><a href="tel:+1 (805) 245 0298">+1&#160;(805)&#160;245&#160;0298</a></p>
<p style="position:absolute;top:65px;left:266px;white-space:nowrap" class="ft01">|&#160;<a href="mailto:jiayi_tian@ucsb.edu">jiayi_tian@ucsb.edu&#160;</a>|&#160;<a href="https://github.com/ttttttris">github.com/ttttttris&#160;</a>|</p>
<p style="position:absolute;top:65px;left:560px;white-space:nowrap" class="ft01"><a href="https://www.linkedin.com/in/jiayi-tian-32b9652a5/">linkedin.com/in/jiayi-tian-32b9652a5/</a></p>
<p style="position:absolute;top:94px;left:64px;white-space:nowrap" class="ft02"><b>Focus&#160;on&#160;eﬀicient&#160;training&#160;and&#160;inference&#160;of&#160;LLMs&#160;(Low-rank&#160;decomposition,&#160;Pruning,&#160;Quantization,&#160;Knowledge&#160;Distillation)</b></p>
<p style="position:absolute;top:133px;left:64px;white-space:nowrap" class="ft03"><b>EDUCATION</b></p>
<p style="position:absolute;top:153px;left:64px;white-space:nowrap" class="ft04"><b>University&#160;of&#160;California,&#160;Santa&#160;Barbara</b></p>
<p style="position:absolute;top:154px;left:341px;white-space:nowrap" class="ft05">,</p>
<p style="position:absolute;top:153px;left:350px;white-space:nowrap" class="ft06"><i>Ph.D.&#160;in&#160;Computer&#160;Engineering</i></p>
<p style="position:absolute;top:154px;left:564px;white-space:nowrap" class="ft05">|&#160;CA,&#160;USA</p>
<p style="position:absolute;top:153px;left:638px;white-space:nowrap" class="ft04"><b>3.9/4.0</b></p>
<p style="position:absolute;top:155px;left:773px;white-space:nowrap" class="ft07">Fall&#160;2023&#160;-&#160;ongoing</p>
<p style="position:absolute;top:171px;left:64px;white-space:nowrap" class="ft04"><b>Nanjing&#160;University</b></p>
<p style="position:absolute;top:171px;left:196px;white-space:nowrap" class="ft05">,</p>
<p style="position:absolute;top:171px;left:205px;white-space:nowrap" class="ft06"><i>B.Eng.&#160;in&#160;VLSI&#160;Design&#160;&amp;&#160;System&#160;Integration</i></p>
<p style="position:absolute;top:171px;left:503px;white-space:nowrap" class="ft05">|&#160;China</p>
<p style="position:absolute;top:171px;left:553px;white-space:nowrap" class="ft04"><b>4.5/5.0</b></p>
<p style="position:absolute;top:173px;left:771px;white-space:nowrap" class="ft07">Fall&#160;2019&#160;-&#160;Jun&#160;2023</p>
<p style="position:absolute;top:212px;left:64px;white-space:nowrap" class="ft03"><b>INDUSTRIAL&#160;EXPERIENCE</b></p>
<p style="position:absolute;top:238px;left:64px;white-space:nowrap" class="ft04"><b>Intel&#160;Corporation,</b></p>
<p style="position:absolute;top:238px;left:191px;white-space:nowrap" class="ft06"><i>Research&#160;Intern</i></p>
<p style="position:absolute;top:238px;left:294px;white-space:nowrap" class="ft05">|&#160;Hillsboro,&#160;OR</p>
<p style="position:absolute;top:239px;left:724px;white-space:nowrap" class="ft08">June.&#160;2025&#160;-&#160;ongoing</p>
<p style="position:absolute;top:257px;left:58px;white-space:nowrap" class="ft09">•&#160;Working&#160;on&#160;eﬀicient&#160;chain-of-thoughts&#160;reasoning&#160;via&#160;KV&#160;cache&#160;compression,&#160;leveraging&#160;methods&#160;including&#160;token</p>
<p style="position:absolute;top:272px;left:77px;white-space:nowrap" class="ft09">eviction,&#160;KV&#160;sharing,&#160;early&#160;exit,&#160;and&#160;activation&#160;steering.</p>
<p style="position:absolute;top:300px;left:64px;white-space:nowrap" class="ft04"><b>Intel&#160;Corporation,</b></p>
<p style="position:absolute;top:300px;left:191px;white-space:nowrap" class="ft06"><i>Research&#160;Intern</i></p>
<p style="position:absolute;top:300px;left:294px;white-space:nowrap" class="ft05">|&#160;Hillsboro,&#160;OR</p>
<p style="position:absolute;top:301px;left:711px;white-space:nowrap" class="ft08">June.&#160;2024&#160;-&#160;Sep.&#160;2024</p>
<p style="position:absolute;top:320px;left:58px;white-space:nowrap" class="ft09">•&#160;Proposed&#160;a&#160;tensor-compressed&#160;LLM&#160;training&#160;accelerator&#160;using&#160;FPGA&#160;with&#160;optimized&#160;compute&#160;ordering,&#160;dataflow,</p>
<p style="position:absolute;top:335px;left:77px;white-space:nowrap" class="ft09">and&#160;memory&#160;allocation,&#160;resulting&#160;paper&#160;accepted&#160;to&#160;IEEE&#160;TCAD.</p>
<p style="position:absolute;top:351px;left:58px;white-space:nowrap" class="ft09">•&#160;Achieved&#160;up&#160;to</p>
<p style="position:absolute;top:351px;left:186px;white-space:nowrap" class="ft010">48×</p>
<p style="position:absolute;top:351px;left:219px;white-space:nowrap" class="ft09">memory&#160;eﬀiciency&#160;and</p>
<p style="position:absolute;top:351px;left:378px;white-space:nowrap" class="ft010">3.6×</p>
<p style="position:absolute;top:351px;left:415px;white-space:nowrap" class="ft09">energy&#160;eﬀiciency&#160;compared&#160;to&#160;Nvidia&#160;RTX&#160;3090.</p>
<p style="position:absolute;top:378px;left:64px;white-space:nowrap" class="ft04"><b>AMD-Xilinx&#160;Technology,</b></p>
<p style="position:absolute;top:378px;left:234px;white-space:nowrap" class="ft06"><i>Co-Op/Intern</i></p>
<p style="position:absolute;top:379px;left:324px;white-space:nowrap" class="ft05">|&#160;Beijing,&#160;China</p>
<p style="position:absolute;top:380px;left:722px;white-space:nowrap" class="ft08">June&#160;2023&#160;-&#160;Sep&#160;2023</p>
<p style="position:absolute;top:398px;left:58px;white-space:nowrap" class="ft09">•&#160;Developed&#160;a&#160;C++/HLS&#160;Transformer&#160;training&#160;framework&#160;with&#160;custom&#160;tensorized&#160;linear&#160;layers&#160;and&#160;nonlinear&#160;oper-</p>
<p style="position:absolute;top:415px;left:77px;white-space:nowrap" class="ft09">ations&#160;for&#160;LLM&#160;acceleration,&#160;achieved</p>
<p style="position:absolute;top:415px;left:341px;white-space:nowrap" class="ft010">30×&#160;∼&#160;52×</p>
<p style="position:absolute;top:415px;left:423px;white-space:nowrap" class="ft09">saving&#160;in&#160;model&#160;size&#160;for&#160;end-to-end&#160;Transformer&#160;training.</p>
<p style="position:absolute;top:458px;left:64px;white-space:nowrap" class="ft03"><b>SKILLS&#160;&amp;&#160;RESEARCH&#160;INTERESTS</b></p>
<p style="position:absolute;top:479px;left:64px;white-space:nowrap" class="ft02"><b>Languages&#160;&amp;&#160;Tools</b></p>
<p style="position:absolute;top:479px;left:199px;white-space:nowrap" class="ft011">Python,&#160;PyTorch,&#160;TensorFlow,&#160;Huggingface,&#160;C/C++,&#160;High-level&#160;Synthesis&#160;(HLS),&#160;Vivado/Vitis/XRT</p>
<p style="position:absolute;top:506px;left:64px;white-space:nowrap" class="ft02"><b>ML&#160;&amp;&#160;NLP</b></p>
<p style="position:absolute;top:497px;left:199px;white-space:nowrap" class="ft014">Large&#160;Language&#160;Models&#160;(LLMs),&#160;Eﬀicient&#160;Training/Inference&#160;(Model&#160;Compression,&#160;KV&#160;Cache&#160;Compression,<br/>Pruning,&#160;Low-rank&#160;decomposition,&#160;Distillation,&#160;Quantization)</p>
<p style="position:absolute;top:554px;left:64px;white-space:nowrap" class="ft03"><b>PUBLICATIONS&#160;&amp;&#160;PREPRINTS</b></p>
<p style="position:absolute;top:589px;left:64px;white-space:nowrap" class="ft012"><b>FLAT-LLM:&#160;Fine-grained&#160;Low-rank&#160;Activation&#160;Space&#160;Transformation&#160;for&#160;Large&#160;Language&#160;Model&#160;Compression</b></p>
<p style="position:absolute;top:607px;left:64px;white-space:nowrap" class="ft013">Jiayi&#160;Tian,&#160;Ryan&#160;Solgi,&#160;Jinming&#160;Lu,&#160;Yifan&#160;Yang,&#160;Hai&#160;Li,&#160;Zheng&#160;Zhang,&#160;under&#160;review&#160;at&#160;ARR&#160;July,&#160;2025.&#160;<a href="https://arxiv.org/pdf/2505.23966">arXiv&#160;preprint.</a></p>
<p style="position:absolute;top:634px;left:64px;white-space:nowrap" class="ft012"><b>FETTA:&#160;Flexible&#160;and&#160;Eﬀicient&#160;Hardware&#160;Accelerator&#160;for&#160;Tensorized&#160;Neural&#160;Network&#160;Training</b></p>
<p style="position:absolute;top:652px;left:64px;white-space:nowrap" class="ft015">Jinming&#160;Lu,&#160;Jiayi&#160;Tian,&#160;Hai&#160;Li,&#160;Ian&#160;Young,&#160;Zheng&#160;Zhang,&#160;under&#160;review&#160;at&#160;IEEE&#160;Transactions&#160;on&#160;Computer-Aided&#160;Design&#160;of&#160;Integrated<br/>Circuits&#160;and&#160;Systems.&#160;<a href="https://arxiv.org/pdf/2504.06474">arXiv&#160;preprint</a>.</p>
<p style="position:absolute;top:697px;left:64px;white-space:nowrap" class="ft012"><b>Ultra&#160;Memory-Eﬀicient&#160;On-FPGA&#160;Training&#160;of&#160;Transformers&#160;via&#160;Tensor-Compressed&#160;Optimization</b></p>
<p style="position:absolute;top:715px;left:64px;white-space:nowrap" class="ft015">Jiayi&#160;Tian,&#160;Jinming&#160;Lu,&#160;Hai&#160;Li,&#160;Xiangwei&#160;Wang,&#160;Cong&#160;(Callie)&#160;Hao,&#160;Ian&#160;Young,&#160;Zheng&#160;Zhang,&#160;accepted&#160;to&#160;IEEE&#160;Transactions&#160;on<br/>Computer-Aided&#160;Design&#160;of&#160;Integrated&#160;Circuits&#160;and&#160;Systems.&#160;<a href="https://arxiv.org/pdf/2501.06663">arXiv&#160;preprint</a>.</p>
<p style="position:absolute;top:759px;left:64px;white-space:nowrap" class="ft012"><b>BEBERT:&#160;Eﬀicient&#160;and&#160;robust&#160;binary&#160;ensemble&#160;BERT</b></p>
<p style="position:absolute;top:778px;left:64px;white-space:nowrap" class="ft015">Tian,&#160;Jiayi,&#160;Chao&#160;Fang,&#160;Haonan&#160;Wang,&#160;and&#160;Zhongfeng&#160;Wang,&#160;ICASSP&#160;2023-2023&#160;IEEE&#160;International&#160;Conference&#160;on&#160;Acoustics,<br/>Speech&#160;and&#160;Signal&#160;Processing&#160;(ICASSP).&#160;IEEE,&#160;2023.</p>
<p style="position:absolute;top:834px;left:64px;white-space:nowrap" class="ft03"><b>RESEARCH&#160;PROJECTS</b></p>
<p style="position:absolute;top:863px;left:64px;white-space:nowrap" class="ft04"><b>Structural&#160;Pruning&#160;for&#160;Eﬀicient&#160;LLM&#160;Inference&#160;via&#160;Low-rank&#160;Decomposition</b></p>
<p style="position:absolute;top:864px;left:711px;white-space:nowrap" class="ft08">Aug.&#160;2024&#160;-&#160;May.&#160;2025</p>
<p style="position:absolute;top:883px;left:58px;white-space:nowrap" class="ft09">•&#160;Developed&#160;a&#160;training-free,&#160;fine-grained&#160;compression&#160;method&#160;that&#160;leverages&#160;the&#160;low-rank&#160;structure&#160;of&#160;the&#160;activation</p>
<p style="position:absolute;top:898px;left:77px;white-space:nowrap" class="ft09">space&#160;to&#160;transform&#160;and&#160;compress&#160;the&#160;model&#160;weights.</p>
<p style="position:absolute;top:914px;left:58px;white-space:nowrap" class="ft09">•&#160;Introduced&#160;a&#160;novel&#160;training-free&#160;rank&#160;selection&#160;algorithm&#160;that&#160;allocates&#160;ranks&#160;using&#160;a&#160;greedy&#160;redistribution&#160;strategy</p>
<p style="position:absolute;top:929px;left:77px;white-space:nowrap" class="ft09">and&#160;can&#160;be&#160;integrated&#160;with&#160;existing&#160;low-rank&#160;LLM&#160;compression&#160;pipelines.</p>
<p style="position:absolute;top:947px;left:58px;white-space:nowrap" class="ft09">•&#160;Achieved&#160;strong&#160;performance&#160;on&#160;LLaMA-2,&#160;3&#160;and&#160;Mistral&#160;models&#160;with&#160;minimal&#160;calibration&#160;overhead&#160;(within</p>
<p style="position:absolute;top:965px;left:77px;white-space:nowrap" class="ft09">minutes),&#160;validated&#160;across&#160;language&#160;modeling&#160;and&#160;downstream&#160;tasks.</p>
<p style="position:absolute;top:994px;left:64px;white-space:nowrap" class="ft04"><b>Training&#160;Accelerator&#160;Design&#160;for&#160;Tensor-Compressed&#160;Transformer&#160;Models</b></p>
<p style="position:absolute;top:995px;left:717px;white-space:nowrap" class="ft08">Sep.&#160;2023&#160;-&#160;Dec.&#160;2024</p>
<p style="position:absolute;top:1014px;left:58px;white-space:nowrap" class="ft09">•&#160;Designed&#160;a&#160;tensor-compressed&#160;training&#160;scheme&#160;for&#160;Transformer&#160;models&#160;that&#160;reduces&#160;model&#160;size&#160;by</p>
<p style="position:absolute;top:1014px;left:755px;white-space:nowrap" class="ft010">30&#160;∼&#160;52×</p>
<p style="position:absolute;top:1014px;left:819px;white-space:nowrap" class="ft09">.</p>
<p style="position:absolute;top:1030px;left:58px;white-space:nowrap" class="ft09">•&#160;Introduced&#160;bidirectional&#160;tensor&#160;contraction&#160;to&#160;enhance&#160;memory&#160;and&#160;compute&#160;eﬀiciency,&#160;especially&#160;in&#160;long-sequence</p>
<p style="position:absolute;top:1045px;left:77px;white-space:nowrap" class="ft09">training&#160;and&#160;inference.</p>
<p style="position:absolute;top:1062px;left:58px;white-space:nowrap" class="ft09">•&#160;Built&#160;an&#160;HLS-based&#160;Transformer&#160;training&#160;engine&#160;achieving&#160;up&#160;to</p>
<p style="position:absolute;top:1062px;left:518px;white-space:nowrap" class="ft010">48×</p>
<p style="position:absolute;top:1062px;left:550px;white-space:nowrap" class="ft09">memory&#160;eﬀiciency&#160;and</p>
<p style="position:absolute;top:1062px;left:706px;white-space:nowrap" class="ft010">3.6×</p>
<p style="position:absolute;top:1062px;left:743px;white-space:nowrap" class="ft09">energy&#160;eﬀiciency</p>
<p style="position:absolute;top:1077px;left:77px;white-space:nowrap" class="ft09">compared&#160;with&#160;Nvidia&#160;RTX&#160;3090.</p>
</div>
</body>
</html>
